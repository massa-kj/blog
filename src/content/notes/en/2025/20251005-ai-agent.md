---
title: "AI Agent Architecture and Components"
date: "2025-10-05"
tags: ["ai", "agent", "llm", "mcp", "rag"]
summary: "Understanding the architecture, components, and processing flow of AI agents built around LLMs"
---

## How AI Agents Work with LLMs

- AI Utilization in Development
  - Through using AI agents in work, I've experienced firsthand how they can significantly boost both productivity and learning efficiency when used effectively.
  - The advantage of partial coding skills is likely to diminish over time.
  - AI utilization will become a prerequisite in development. How we leverage AI and understanding AI agent architecture will greatly impact an engineer's capabilities and growth.
- What is an AI Agent
  - ChatGPT and GitHub Copilot are AI agents. They are built around LLMs as the core, analyze user input, and perform tasks by utilizing external tools and knowledge as needed.
  - The basic processing flow of AI agents involves receiving user input, generating output or processing tasks using LLM, RAG, MCP, and other components.
  - AI agents differ significantly based on model selection, how RAG and caching are implemented and used, MCP utilization, and where LLMs are applied.
- AI Agent Components Example
  ```mermaid
  flowchart TD
    subgraph Orchestrator["ðŸŽ¯ Agent / Client (Orchestrator)"]
      Planner["ðŸ—‚ï¸ Planner (Control & Coordination)"]
      Memory["ðŸ§¾ Memory (Context & State Management)"]
      LLM["ðŸ§  LLM (Thinking & Judgment & Generation)"]
    end
  
    subgraph External["ðŸ“¡ External Extension Layer"]
      RAG["ðŸ“š RAG (Knowledge Retrieval)"]
      MCP["ðŸ›  MCP (Tool Invocation Protocol)"]
    end
  
    User["ðŸ§‘ User Input"]
    Output["ðŸ—£ Response / Execution Result"]
  
    User --> Planner
    Planner --> LLM
    LLM -->|Output| Planner
  
    Planner -->|As needed| RAG
    Planner -->|As needed| MCP
    RAG --> Planner
    MCP --> Planner
  
    Planner <--> Memory
    LLM <--> Memory
  
    Planner --> Output
  ```
  - Orchestrator (Agent / Client)
    | Component                     | Main Responsibilities                                                                       | Input                                      | Output                                    | Notes                                    |
    | ----------------------------- | ------------------------------------------------------------------------------------------- | ------------------------------------------ | ----------------------------------------- | ---------------------------------------- |
    | Planner (Control & Coordination) | Analyzes user intent and determines which procedures and tools to use. Controls LLM-based thinking and tool invocation. | User input, Memory state, LLM output      | Execution plans, LLM call instructions, RAG/MCP calls | "Command center of the brain". Core Agent logic. |
    | LLM (Thinking & Judgment & Generation) | Performs reasoning, summarization, explanation, and judgment, generating text.              | Prompts from Planner, knowledge from Memory/RAG | Text responses, tool invocation suggestions | "Thinking engine". Can provide autonomous suggestions. |
    | Memory (Context & State Management) | Maintains past conversations, execution states, and intermediate results, providing them to Planner and LLM as needed. | Write requests from Planner/LLM           | Stored contextual information             | "Short-term memory" or "working notes". Dynamic, unlike RAG. |
  
  - External Extension Layer
    | Component          | Main Responsibilities                                                    | Input                     | Output                      | Notes                        |
    | ------------------ | ------------------------------------------------------------------------ | ------------------------- | --------------------------- | ---------------------------- |
    | RAG (Knowledge Retrieval) | Searches for relevant knowledge from local or external databases to assist LLM generation. | Query (generated by LLM/Planner) | Related documents, excerpts, summaries | Equivalent to "long-term memory". Static knowledge base. |
    | MCP (Tool Invocation) | Abstracts and provides command execution, API calls, and external service integration. | Execution commands (from Planner) | Execution results, error messages | "Action arms". Interface to the world. |
  
  - Supplementary Concepts
    | Concept           | Overview                                     | Example                                |
    | ----------------- | -------------------------------------------- | -------------------------------------- |
    | User (Input)      | Natural language or structured commands as input. | "List S3 buckets"                     |
    | Output (Response) | Final output from the agent. Execution results or summaries. | "Here is the S3 bucket list..."       |
    | Context Flow      | Context sharing between Planner â‡„ LLM â‡„ Memory | "Linking to past questions", "Reusing previous output", etc. |
    | Tool Invocation   | Planner â‡„ MCP or RAG calls                  | "API calls", "File searches", etc.    |
  
  - Processing Flow (Overview)
    1. Receive User input â†’ Planner analyzes intent
    2. Planner decides whether to "make LLM think", "search with RAG", "execute with MCP", etc.
    3. LLM performs thinking and generation, returns to Planner
    4. Planner calls external tools as needed
    5. Save conversation and state to Memory (maintain context for future sessions)
    6. Return final output to user

- Role and Limitations of LLMs
  - LLMs alone are just text generation models. They cannot perform actual environment operations or task execution.
  - However, LLMs can be used to judge whether RAG or MCP tools should be utilized (Function Calling / Tool Use functionality).
  - In agent processing, LLMs are used for command generation, selection judgment, and summarization.
  - Latest models (GPT-4, Claude 3.5 Sonnet, etc.) are also good at generating structured output (JSON format, etc.).

- Utilizing MCP (Model Context Protocol)
  - Basic Concept: A protocol developed by Anthropic that serves as a standard specification for AI models to safely integrate with external tools and resources.
  - Functionality Extension: MCP enables LLM-based AI agents to execute actual task processing (file operations, API calls, database access, etc.).
  - Security: Provides safe external system integration through sandboxed tool execution environments.
  - Implementation Examples:
    - ChatGPT and GitHub Copilot can utilize MCP servers
    - Can build custom MCP servers for use as custom tools
    - In VS Code, MCP server programs are launched and managed through `settings.json` configuration
      - Like LSP (Language Server Protocol), uses lightweight communication via standard input/output
  - Use Cases: File system operations, Git operations, database queries, Web API calls, code execution, etc.

- RAG (Retrieval-Augmented Generation) Details
  - Concept: Retrieval-Augmented Generation. A technique that combines LLM generation capabilities with external knowledge retrieval.
  - Processing Steps:
    1. Indexing: Vectorize documents and store them in a database
    2. Retrieval: Search for documents similar to user queries
    3. Generation: Provide search results as context to LLM for answer generation
  - Technical Components:
    - Embedding Model: Converts text to vector representations (OpenAI text-embedding-ada-002, Sentence Transformers, etc.)
    - Vector Database: Enables high-speed similarity searches (Pinecone, Weaviate, Chroma, etc.)
    - Chunking Strategy: Methods for splitting long documents into appropriate sizes
  - Benefits: Reflecting latest information, utilizing specialized knowledge, suppressing hallucinations

## Real AI Agent Examples

- Development Support:
  - GitHub Copilot: Code completion, file operations via MCP, Git integration
  - Cursor: Understanding entire codebase, related code search via RAG
  - v0.dev (Vercel): UI component generation, integration with preview functionality

- General Purpose:
  - ChatGPT: Code Interpreter, plugin system, image analysis
  - Claude: Artifacts functionality, MCP server integration
  - Google Bard/Gemini: Google Workspace integration, real-time information search

- Specialized:
  - Devin: Fully autonomous software engineer AI agent
  - AutoGPT: Autonomous processing from goal setting to execution
  - LangChain Agents: Customizable agent framework

## Not Yet Investigated

- Cutting-edge AI agent development beyond LLMs
- Importance of prompt engineering
- AI agent evaluation and measurement methods
- Security and privacy considerations
- Cost optimization strategies
